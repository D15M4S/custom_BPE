# BPE í•™ìŠµ ê³„íšì„œ (ì™„ì „ ì´ˆë³´ììš©)

> "ì²œ ë¦¬ ê¸¸ë„ í•œ ê±¸ìŒë¶€í„°" - ì´ ê³„íšì„œëŠ” BPEì— ëŒ€í•´ ì•„ë¬´ê²ƒë„ ëª¨ë¥´ëŠ” ë¶„ë“¤ì„ ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.
> ì²œì²œíˆ, í•˜ë‚˜ì”© ë°°ì›Œë‚˜ê°€ë©´ ë©ë‹ˆë‹¤. ì„œë‘ë¥´ì§€ ë§ˆì„¸ìš”!

## ğŸ“‹ ëª©ì°¨
1. [í•™ìŠµ ì „ ì¤€ë¹„ì‚¬í•­](#í•™ìŠµ-ì „-ì¤€ë¹„ì‚¬í•­)
2. [í•™ìŠµ ë‹¨ê³„ ê°œìš”](#í•™ìŠµ-ë‹¨ê³„-ê°œìš”)
3. [Week 1: BPE ê¸°ì´ˆ ì´í•´](#week-1-bpe-ê¸°ì´ˆ-ì´í•´)
4. [Week 2: ì•Œê³ ë¦¬ì¦˜ ì›ë¦¬ ê¹Šì´ ì´í•´](#week-2-ì•Œê³ ë¦¬ì¦˜-ì›ë¦¬-ê¹Šì´-ì´í•´)
5. [Week 3-4: ê¸°ë³¸ BPE êµ¬í˜„](#week-3-4-ê¸°ë³¸-bpe-êµ¬í˜„)
6. [Week 5: ê³ ê¸‰ ê¸°ëŠ¥ êµ¬í˜„](#week-5-ê³ ê¸‰-ê¸°ëŠ¥-êµ¬í˜„)
7. [ì‹¤ìŠµ ì˜ˆì œ](#ì‹¤ìŠµ-ì˜ˆì œ)
8. [ì°¸ê³  ìë£Œ](#ì°¸ê³ -ìë£Œ)
9. [í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸](#í•™ìŠµ-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## í•™ìŠµ ì „ ì¤€ë¹„ì‚¬í•­

### í•„ìš”í•œ ì‚¬ì „ ì§€ì‹
- [ ] Python ê¸°ë³¸ ë¬¸ë²• (ë³€ìˆ˜, ë°˜ë³µë¬¸, ì¡°ê±´ë¬¸, í•¨ìˆ˜)
- [ ] Python ìë£Œêµ¬ì¡° (ë¦¬ìŠ¤íŠ¸, ë”•ì…”ë„ˆë¦¬)
- [ ] ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ ì´í•´ (ë°˜ë³µ, ì •ë ¬ ê°œë…)

### í•„ìš”í•œ ë„êµ¬
- [ ] Python 3.8 ì´ìƒ
- [ ] ì½”ë“œ ì—ë””í„° (VS Code ì¶”ì²œ)
- [ ] Git (ì„ íƒì‚¬í•­)

### í•™ìŠµ ì‹œê°„ ì˜ˆìƒ
- **ì´ í•™ìŠµ ê¸°ê°„**: 5ì£¼ (ì£¼ë‹¹ 5-7ì‹œê°„)
- **ë§¤ì¼ í•™ìŠµ ê¶Œì¥**: 1-1.5ì‹œê°„
- **ì£¼ë§ ë³µìŠµ**: 2-3ì‹œê°„

### í•™ìŠµ ë°©ë²•
1. **ì´í•´ â†’ ì—°ìŠµ â†’ êµ¬í˜„** ìˆœì„œë¥¼ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”
2. **ê° ë‹¨ê³„ë¥¼ ì™„ì „íˆ ì´í•´í•œ í›„** ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°€ì„¸ìš”
3. **ì§ì ‘ ì½”ë“œë¥¼ íƒ€ì´í•‘**í•˜ì„¸ìš” (ë³µì‚¬-ë¶™ì—¬ë„£ê¸° ê¸ˆì§€!)
4. **ë§‰íˆë©´ ì‰¬ì–´ê°€ì„¸ìš”** - í•™ìŠµì€ ë§ˆë¼í†¤ì…ë‹ˆë‹¤

---

## í•™ìŠµ ë‹¨ê³„ ê°œìš”

```
[Week 1] ê°œë… ì´í•´
    â†“
[Week 2] ì•Œê³ ë¦¬ì¦˜ ì´í•´
    â†“
[Week 3] ê¸°ë³¸ êµ¬í˜„ (base.py)
    â†“
[Week 4] í•µì‹¬ êµ¬í˜„ (basic.py)
    â†“
[Week 5] ê³ ê¸‰ ê¸°ëŠ¥ (regex.py)
```

---

## Week 1: BPE ê¸°ì´ˆ ì´í•´

### ğŸ“… Day 1-2: "í† í¬ë‚˜ì´ì œì´ì…˜ì´ ë­”ê°€ìš”?"

#### í•™ìŠµ ëª©í‘œ
- í† í¬ë‚˜ì´ì œì´ì…˜(Tokenization)ì´ ë¬´ì—‡ì¸ì§€ ì´í•´í•˜ê¸°
- ì™œ í† í¬ë‚˜ì´ì œì´ì…˜ì´ í•„ìš”í•œì§€ ì•Œê¸°

#### ê°œë… ì„¤ëª…

**í† í¬ë‚˜ì´ì œì´ì…˜ì´ë€?**
- ë¬¸ì¥ì„ ì‘ì€ ì¡°ê°(í† í°)ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì…ë‹ˆë‹¤
- ë¹„ìœ : ë ˆê³  ë¸”ë¡ì²˜ëŸ¼ í° êµ¬ì¡°ë¬¼(ë¬¸ì¥)ì„ ì‘ì€ ë¸”ë¡(í† í°)ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ê²ƒ

**ì˜ˆì‹œ 1: ê³µë°± ê¸°ë°˜ í† í¬ë‚˜ì´ì œì´ì…˜**
```python
ë¬¸ì¥ = "ë‚˜ëŠ” í•™êµì— ê°‘ë‹ˆë‹¤"
í† í°ë“¤ = ["ë‚˜ëŠ”", "í•™êµì—", "ê°‘ë‹ˆë‹¤"]  # ê³µë°±ìœ¼ë¡œ ë‚˜ëˆ”
```

**ì˜ˆì‹œ 2: ë¬¸ì ê¸°ë°˜ í† í¬ë‚˜ì´ì œì´ì…˜**
```python
ë¬¸ì¥ = "ì•ˆë…•"
í† í°ë“¤ = ["ì•ˆ", "ë…•"]  # ê° ë¬¸ìë¥¼ í† í°ìœ¼ë¡œ
```

#### ì‹¤ìŠµ ê³¼ì œ
```python
# ì§ì ‘ í•´ë³´ì„¸ìš”!
# 1. ë‹¤ìŒ ë¬¸ì¥ì„ ê³µë°±ìœ¼ë¡œ ë‚˜ëˆ ë³´ì„¸ìš”
sentence = "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ìš”"
tokens = sentence.split()
print(tokens)

# 2. ê° ë¬¸ìë¡œ ë‚˜ëˆ ë³´ì„¸ìš”
sentence = "íŒŒì´ì¬"
tokens = list(sentence)
print(tokens)

# ì§ˆë¬¸: ë‘ ë°©ë²•ì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] í† í¬ë‚˜ì´ì œì´ì…˜ì´ ë¬´ì—‡ì¸ì§€ ìì‹ ì˜ ë§ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?
- [ ] ê³µë°± ê¸°ë°˜ê³¼ ë¬¸ì ê¸°ë°˜ì˜ ì°¨ì´ë¥¼ ì´í•´í–ˆë‚˜ìš”?
- [ ] ì‹¤ìŠµ ì½”ë“œë¥¼ ì§ì ‘ ì‘ì„±í•˜ê³  ì‹¤í–‰í–ˆë‚˜ìš”?

---

### ğŸ“… Day 3-4: "BPEê°€ ë­”ê°€ìš”?"

#### í•™ìŠµ ëª©í‘œ
- BPEì˜ ì •ì˜ì™€ ì—­ì‚¬ ì´í•´í•˜ê¸°
- BPEê°€ í•´ê²°í•˜ëŠ” ë¬¸ì œ ì•Œê¸°

#### ê°œë… ì„¤ëª…

**BPEì˜ íƒ„ìƒ ë°°ê²½**
- ì›ë˜ëŠ” **ë°ì´í„° ì••ì¶•** ì•Œê³ ë¦¬ì¦˜ (1994ë…„)
- "ìì£¼ ë‚˜ì˜¤ëŠ” íŒ¨í„´ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ë©´ ê³µê°„ì„ ì•„ë‚„ ìˆ˜ ìˆì–´!"

**ì™œ NLPì—ì„œ BPEë¥¼ ì“¸ê¹Œ?**

**ë¬¸ì œ 1: OOV (Out-Of-Vocabulary) - ëª¨ë¥´ëŠ” ë‹¨ì–´ ë¬¸ì œ**
```
ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´: "ìŠˆí¼ì»´í“¨í„°"
ê³µë°± ê¸°ë°˜ í† í¬ë‚˜ì´ì €: "???" (ëª¨ë¦„)
BPE í† í¬ë‚˜ì´ì €: ["ìŠˆí¼", "ì»´í“¨í„°"] (ì•Œê³  ìˆëŠ” ì¡°ê°ìœ¼ë¡œ ë¶„í•´!)
```

**ë¬¸ì œ 2: ì–´íœ˜ í¬ê¸° í­ë°œ**
```
ë‹¨ì–´ ê¸°ë°˜: 10ë§Œ ê°œ ì´ìƒì˜ ë‹¨ì–´ í•„ìš”
BPE: 3ë§Œ-5ë§Œ ê°œì˜ ì„œë¸Œì›Œë“œë¡œ í•´ê²°
```

#### ì‹¤ìƒí™œ ë¹„ìœ 

**ë ˆì‹œí”¼ ì¡°í•© ë¹„ìœ :**
```
ì¬ë£Œ: [ë°€ê°€ë£¨, ë¬¼, ì„¤íƒ•, ê³„ë€, ìš°ìœ ]

ìì£¼ í•¨ê»˜ ì“°ëŠ” ì¡°í•© ë°œê²¬:
- ë°€ê°€ë£¨ + ë¬¼ â†’ "ë°˜ì£½"
- ì„¤íƒ• + ê³„ë€ â†’ "ì„¤íƒ•ê³„ë€ë¬¼"

ì´ì œ ìƒˆë¡œìš´ ì–´íœ˜:
[ë°˜ì£½, ì„¤íƒ•ê³„ë€ë¬¼, ìš°ìœ ]

ë³µì¡í•œ ë ˆì‹œí”¼ë„ ì´ ì¡°í•©ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥!
```

#### ì‹¤ìŠµ ê³¼ì œ
```python
# ì§ì ‘ ìƒê°í•´ë³´ì„¸ìš”!

# 1. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë¬¸ì ìŒì„ ì°¾ì•„ë³´ì„¸ìš”
text = "aaabbb aaabbb aaabbb ccc"

# íŒíŠ¸: 'aa', 'ab', 'bb' ì¤‘ ë­ê°€ ê°€ì¥ ë§ì´ ë‚˜ì˜¬ê¹Œìš”?
# ì¢…ì´ì— ì§ì ‘ ì„¸ì–´ë³´ì„¸ìš”!

# 2. ì™œ ê·¸ ìŒì´ ê°€ì¥ ë§ì´ ë‚˜ì™”ì„ê¹Œìš”?
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] BPEê°€ ë¬´ì—‡ì˜ ì•½ìì¸ì§€ ì•Œê³  ìˆë‚˜ìš”?
- [ ] OOV ë¬¸ì œê°€ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?
- [ ] BPEê°€ ì–´ë–»ê²Œ OOV ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ì§€ ì´í•´í–ˆë‚˜ìš”?

---

### ğŸ“… Day 5-7: "BPEëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?"

#### í•™ìŠµ ëª©í‘œ
- BPE ì•Œê³ ë¦¬ì¦˜ì˜ ì „ì²´ íë¦„ ì´í•´í•˜ê¸°
- ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ BPE ê³¼ì • ë”°ë¼ê°€ê¸°

#### BPE ì•Œê³ ë¦¬ì¦˜ 4ë‹¨ê³„

```
[1ë‹¨ê³„] ì¤€ë¹„
    â†“
[2ë‹¨ê³„] ë¹ˆë„ ê³„ì‚°
    â†“
[3ë‹¨ê³„] ë³‘í•©
    â†“
[4ë‹¨ê³„] ë°˜ë³µ
```

#### ì´ˆê°„ë‹¨ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°

**ì…ë ¥ í…ìŠ¤íŠ¸:**
```
"low low low lower"
```

**[1ë‹¨ê³„] ì¤€ë¹„ - ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„í•´**
```python
ì´ˆê¸° í† í°ë“¤:
['l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r']

ì´ˆê¸° ì–´íœ˜ (Vocabulary):
{'l', 'o', 'w', ' ', 'e', 'r'}  # 6ê°œ
```

**[2ë‹¨ê³„] ë¹ˆë„ ê³„ì‚° - ì–´ë–¤ ìŒì´ ê°€ì¥ ë§ì´ ë‚˜ì˜¤ë‚˜?**
```python
# ì¢…ì´ì— ì§ì ‘ ì„¸ì–´ë´…ì‹œë‹¤!

ë¬¸ì ìŒ     |  ë‚˜ì˜¨ íšŸìˆ˜
-----------|----------
'l' + 'o'  |  4ë²ˆ  â† ê°€ì¥ ë§ë‹¤!
'o' + 'w'  |  4ë²ˆ  â† ê°€ì¥ ë§ë‹¤!
'w' + ' '  |  3ë²ˆ
'w' + 'e'  |  1ë²ˆ
'e' + 'r'  |  1ë²ˆ

# 'lo'ì™€ 'ow' ì¤‘ 'lo'ë¥¼ ë¨¼ì € ë³‘í•©í•œë‹¤ê³  ê°€ì •
```

**[3ë‹¨ê³„] ë³‘í•© - 'l'+'o' â†’ 'lo'**
```python
ë³‘í•© í›„:
['lo', 'w', ' ', 'lo', 'w', ' ', 'lo', 'w', ' ', 'lo', 'w', 'e', 'r']

ì–´íœ˜ ì—…ë°ì´íŠ¸:
{'l', 'o', 'w', ' ', 'e', 'r', 'lo'}  # 7ê°œ (ìƒˆë¡œìš´ í† í° 'lo' ì¶”ê°€!)

ë³‘í•© ê·œì¹™ ì €ì¥:
Rule 1: 'l' + 'o' â†’ 'lo'
```

**[4ë‹¨ê³„] ë°˜ë³µ - ë‹¤ì‹œ 2ë‹¨ê³„ë¡œ**
```python
# ì´ì œ 'lo'ê°€ í•˜ë‚˜ì˜ í† í°ì´ë¯€ë¡œ, ë‹¤ì‹œ ìŒì„ ì„¸ì–´ë´…ë‹ˆë‹¤

ë¬¸ì ìŒ       |  ë‚˜ì˜¨ íšŸìˆ˜
-------------|----------
'lo' + 'w'   |  4ë²ˆ  â† ì´ì œ ì´ê²Œ ê°€ì¥ ë§ë‹¤!
'w' + ' '    |  3ë²ˆ
...

# 'low'ë¥¼ ë³‘í•©!
ë³‘í•© í›„:
['low', ' ', 'low', ' ', 'low', ' ', 'low', 'e', 'r']

Rule 2: 'lo' + 'w' â†’ 'low'
```

**ê³„ì† ë°˜ë³µí•˜ë©´...**
```python
Rule 3: 'low' + 'e' â†’ 'lowe'
Rule 4: 'lowe' + 'r' â†’ 'lower'

ìµœì¢… ì–´íœ˜:
{'l', 'o', 'w', ' ', 'e', 'r', 'lo', 'low', 'lowe', 'lower'}
```

#### ì†ìœ¼ë¡œ ì§ì ‘ í•´ë³´ê¸° (ë§¤ìš° ì¤‘ìš”!)

**ì‹¤ìŠµ: ì¢…ì´ì™€ ì—°í•„ í•„ìˆ˜!**

```python
# ë‹¤ìŒ í…ìŠ¤íŠ¸ë¡œ BPEë¥¼ ì§ì ‘ í•´ë³´ì„¸ìš”
text = "aaa bbb aaa"

# Step 1: ë¬¸ìë¡œ ë¶„í•´í•˜ì„¸ìš”
tokens = ['a', 'a', 'a', ' ', 'b', 'b', 'b', ' ', 'a', 'a', 'a']

# Step 2: í‘œë¥¼ ê·¸ë ¤ì„œ ìŒì„ ì„¸ì–´ë³´ì„¸ìš”
#
# ìŒ      | íšŸìˆ˜
# --------|-----
# 'a'+'a' | ?
# 'a'+' ' | ?
# ...

# Step 3: ê°€ì¥ ë§ì€ ìŒì„ ë³‘í•©í•˜ì„¸ìš”

# Step 4: ë‹¤ì‹œ Step 2ë¡œ!
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] BPEì˜ 4ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ë§í•  ìˆ˜ ìˆë‚˜ìš”?
- [ ] ì¢…ì´ì— ì§ì ‘ ì˜ˆì‹œë¥¼ í’€ì–´ë´¤ë‚˜ìš”? (ë§¤ìš° ì¤‘ìš”!)
- [ ] ë³‘í•© ê·œì¹™ì´ ì™œ í•„ìš”í•œì§€ ì´í•´í–ˆë‚˜ìš”?
- [ ] "ë¹ˆë„ ê¸°ë°˜"ì´ ë¬´ìŠ¨ ëœ»ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?

---

## Week 2: ì•Œê³ ë¦¬ì¦˜ ì›ë¦¬ ê¹Šì´ ì´í•´

### ğŸ“… Day 1-2: "ì¸ì½”ë”©(Encoding) - í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ"

#### í•™ìŠµ ëª©í‘œ
- í›ˆë ¨ëœ BPEë¡œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ë°©ë²• ì´í•´í•˜ê¸°
- ì¸ì½”ë”© ì•Œê³ ë¦¬ì¦˜ ì§ì ‘ ì‹œë®¬ë ˆì´ì…˜í•´ë³´ê¸°

#### ê°œë… ì„¤ëª…

**ì¸ì½”ë”©ì´ë€?**
- ë¬¸ìì—´ì„ í† í° ID(ìˆ«ì) ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •
- BPE í›ˆë ¨ìœ¼ë¡œ ë§Œë“  **ë³‘í•© ê·œì¹™**ì„ **ìˆœì„œëŒ€ë¡œ** ì ìš©

**ì™œ ìˆ«ìë¡œ ë°”ê¾¸ë‚˜ìš”?**
- ì»´í“¨í„°(íŠ¹íˆ ì‹ ê²½ë§)ëŠ” ìˆ«ìë¡œ ê³„ì‚°í•´ìš”
- "ì•ˆë…•" â†’ [243, 156] ê°™ì€ í˜•íƒœë¡œ ë³€í™˜

#### ì¸ì½”ë”© ì•Œê³ ë¦¬ì¦˜

```python
# ì˜ì‚¬ì½”ë“œ (Pseudocode)

def encode(text):
    # 1. í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìë¡œ ë¶„í•´
    tokens = list(text)

    # 2. ë³‘í•© ê·œì¹™ì„ ìˆœì„œëŒ€ë¡œ ì ìš©
    for merge_rule in merge_rules:  # Rule 1, Rule 2, ...
        # 3. ê·œì¹™ì— í•´ë‹¹í•˜ëŠ” ìŒì„ ì°¾ì•„ì„œ ë³‘í•©
        tokens = apply_merge(tokens, merge_rule)

    # 4. ê° í† í°ì„ IDë¡œ ë³€í™˜
    ids = [vocab[token] for token in tokens]

    return ids
```

#### êµ¬ì²´ì ì¸ ì˜ˆì‹œ

**í›ˆë ¨ ê²°ê³¼:**
```python
# Week 1ì—ì„œ ë§Œë“  ë³‘í•© ê·œì¹™
Rule 1: 'l' + 'o' â†’ 'lo'
Rule 2: 'lo' + 'w' â†’ 'low'
Rule 3: 'low' + 'e' â†’ 'lowe'
Rule 4: 'lowe' + 'r' â†’ 'lower'

# ì–´íœ˜ì™€ ID
vocab = {
    'l': 0, 'o': 1, 'w': 2, ' ': 3, 'e': 4, 'r': 5,
    'lo': 6, 'low': 7, 'lowe': 8, 'lower': 9
}
```

**ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì¸ì½”ë”©:**
```python
text = "lower"

# Step 1: ë¬¸ìë¡œ ë¶„í•´
tokens = ['l', 'o', 'w', 'e', 'r']

# Step 2: Rule 1 ì ìš© ('l'+'o' â†’ 'lo')
tokens = ['lo', 'w', 'e', 'r']

# Step 3: Rule 2 ì ìš© ('lo'+'w' â†’ 'low')
tokens = ['low', 'e', 'r']

# Step 4: Rule 3 ì ìš© ('low'+'e' â†’ 'lowe')
tokens = ['lowe', 'r']

# Step 5: Rule 4 ì ìš© ('lowe'+'r' â†’ 'lower')
tokens = ['lower']

# Step 6: í† í°ì„ IDë¡œ ë³€í™˜
ids = [9]  # 'lower'ì˜ IDëŠ” 9

# ê²°ê³¼: "lower" â†’ [9]
```

#### ì‹¤ìŠµ ê³¼ì œ

```python
# ìœ„ì™€ ê°™ì€ ê·œì¹™ìœ¼ë¡œ ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ì„¸ìš”

text = "low"

# ì¢…ì´ì— ë‹¨ê³„ë³„ë¡œ ì ì–´ë³´ì„¸ìš”:
# Step 1: ë¬¸ì ë¶„í•´
tokens = ['l', 'o', 'w']

# Step 2: Rule 1 ì ìš© í›„
tokens = ?

# Step 3: Rule 2 ì ìš© í›„
tokens = ?

# Step 4: ID ë³€í™˜
ids = ?
```

#### ì¤‘ìš”í•œ í¬ì¸íŠ¸

**ê·œì¹™ ì ìš© ìˆœì„œê°€ ì¤‘ìš”í•´ìš”!**
```python
# ì˜ëª»ëœ ë°©ë²• (ê·œì¹™ ìˆœì„œ ë¬´ì‹œ)
text = "lower"
tokens = ['l', 'o', 'w', 'e', 'r']
# Rule 4ë¥¼ ë¨¼ì € ì ìš©? â†’ ë¶ˆê°€ëŠ¥! 'lowe'ê°€ ì•„ì§ ì—†ìŒ

# ì˜¬ë°”ë¥¸ ë°©ë²• (ê·œì¹™ ìˆœì„œ ì¤€ìˆ˜)
# Rule 1 â†’ Rule 2 â†’ Rule 3 â†’ Rule 4 ìˆœì„œëŒ€ë¡œ!
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] ì¸ì½”ë”©ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?
- [ ] ë³‘í•© ê·œì¹™ì„ ì™œ ìˆœì„œëŒ€ë¡œ ì ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì´í•´í–ˆë‚˜ìš”?
- [ ] ì†ìœ¼ë¡œ ì¸ì½”ë”© ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•´ë´¤ë‚˜ìš”?

---

### ğŸ“… Day 3-4: "ë””ì½”ë”©(Decoding) - ìˆ«ìë¥¼ í…ìŠ¤íŠ¸ë¡œ"

#### í•™ìŠµ ëª©í‘œ
- í† í° IDë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²• ì´í•´í•˜ê¸°
- ì¸ì½”ë”©ê³¼ ë””ì½”ë”©ì˜ ì—­ê´€ê³„ ì´í•´í•˜ê¸°

#### ê°œë… ì„¤ëª…

**ë””ì½”ë”©ì´ë€?**
- í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì›ë˜ ë¬¸ìì—´ë¡œ ë³µì›í•˜ëŠ” ê³¼ì •
- ì¸ì½”ë”©ì˜ **ì—­ê³¼ì •** (ë°˜ëŒ€)

**ì¸ì½”ë”© vs ë””ì½”ë”©**
```
ì¸ì½”ë”©:  "hello" â†’ [234, 156]
         ë¬¸ìì—´ â†’ ìˆ«ì

ë””ì½”ë”©:  [234, 156] â†’ "hello"
         ìˆ«ì â†’ ë¬¸ìì—´
```

#### ë””ì½”ë”© ì•Œê³ ë¦¬ì¦˜

```python
# ì˜ì‚¬ì½”ë“œ

def decode(ids):
    # 1. ê° IDë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
    tokens = [reverse_vocab[id] for id in ids]

    # 2. í† í°ë“¤ì„ ì—°ê²°
    text = ''.join(tokens)

    return text
```

#### êµ¬ì²´ì ì¸ ì˜ˆì‹œ

```python
# ì–´íœ˜ (ê°™ì€ ê²ƒ ì‚¬ìš©)
vocab = {
    'l': 0, 'o': 1, 'w': 2, ' ': 3, 'e': 4, 'r': 5,
    'lo': 6, 'low': 7, 'lowe': 8, 'lower': 9
}

# ì—­ë°©í–¥ ì–´íœ˜ (ID â†’ í† í°)
reverse_vocab = {
    0: 'l', 1: 'o', 2: 'w', 3: ' ', 4: 'e', 5: 'r',
    6: 'lo', 7: 'low', 8: 'lowe', 9: 'lower'
}

# ë””ì½”ë”© ì˜ˆì‹œ
ids = [9]

# Step 1: IDë¥¼ í† í°ìœ¼ë¡œ
tokens = ['lower']  # reverse_vocab[9] = 'lower'

# Step 2: í† í° ì—°ê²°
text = 'lower'

# ê²°ê³¼: [9] â†’ "lower"
```

**ë³µì¡í•œ ì˜ˆì‹œ:**
```python
ids = [7, 3, 7, 4, 5]

# Step 1: ê° IDë¥¼ í† í°ìœ¼ë¡œ
# 7 â†’ 'low'
# 3 â†’ ' '
# 7 â†’ 'low'
# 4 â†’ 'e'
# 5 â†’ 'r'
tokens = ['low', ' ', 'low', 'e', 'r']

# Step 2: ì—°ê²°
text = 'low lower'

# ê²°ê³¼: [7, 3, 7, 4, 5] â†’ "low lower"
```

#### ì‹¤ìŠµ ê³¼ì œ

```python
# ê°™ì€ vocabë¥¼ ì‚¬ìš©í•´ì„œ ë””ì½”ë”©í•˜ì„¸ìš”

ids = [6, 2, 3, 4, 5]

# Step 1: ê° IDë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
# 6 â†’ ?
# 2 â†’ ?
# 3 â†’ ?
# 4 â†’ ?
# 5 â†’ ?

tokens = ?

# Step 2: ì—°ê²°
text = ?
```

#### ì¸ì½”ë”©-ë””ì½”ë”© ê²€ì¦

**ì™•ë³µ í…ŒìŠ¤íŠ¸ (Round-trip test)**
```python
# ì›ë³¸ í…ìŠ¤íŠ¸
original = "lower"

# ì¸ì½”ë”©
ids = encode(original)  # [9]

# ë””ì½”ë”©
restored = decode(ids)  # "lower"

# ê²€ì¦
assert original == restored  # ê°™ì•„ì•¼ í•©ë‹ˆë‹¤!
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] ë””ì½”ë”©ì´ ì¸ì½”ë”©ì˜ ì—­ê³¼ì •ì„ì„ ì´í•´í–ˆë‚˜ìš”?
- [ ] reverse_vocabì´ ì™œ í•„ìš”í•œì§€ ì•Œê³  ìˆë‚˜ìš”?
- [ ] ì™•ë³µ í…ŒìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì´í•´í–ˆë‚˜ìš”?

---

### ğŸ“… Day 5-7: "ìë£Œêµ¬ì¡° ì´í•´í•˜ê¸°"

#### í•™ìŠµ ëª©í‘œ
- BPE êµ¬í˜„ì— í•„ìš”í•œ ìë£Œêµ¬ì¡° ì´í•´í•˜ê¸°
- ê° ìë£Œêµ¬ì¡°ê°€ ì™œ í•„ìš”í•œì§€ ì•Œê¸°

#### í•„ìš”í•œ ìë£Œêµ¬ì¡° 3ê°€ì§€

**1. Vocabulary (ì–´íœ˜ ì‚¬ì „)**
```python
# í† í° â†’ ID ë§¤í•‘
vocab = {
    'h': 0,
    'e': 1,
    'l': 2,
    'o': 3,
    'he': 4,      # ë³‘í•©ëœ í† í°
    'llo': 5,     # ë³‘í•©ëœ í† í°
}

# ìš©ë„: ì¸ì½”ë”© ì‹œ í† í°ì„ IDë¡œ ë³€í™˜
token = 'he'
id = vocab[token]  # 4
```

**2. Merge Rules (ë³‘í•© ê·œì¹™)**
```python
# (í† í°1, í† í°2) â†’ ìš°ì„ ìˆœìœ„
merges = {
    ('h', 'e'): 0,      # Rule 1
    ('l', 'l'): 1,      # Rule 2
    ('ll', 'o'): 2,     # Rule 3
}

# ë˜ëŠ” ìˆœì„œê°€ ìˆëŠ” ë¦¬ìŠ¤íŠ¸
merge_list = [
    ('h', 'e'),   # Rule 1
    ('l', 'l'),   # Rule 2
    ('ll', 'o'),  # Rule 3
]

# ìš©ë„: ì¸ì½”ë”© ì‹œ ì–´ë–¤ ìŒì„ ë³‘í•©í• ì§€ ê²°ì •
```

**3. Pair Frequencies (ìŒ ë¹ˆë„)**
```python
# (í† í°1, í† í°2) â†’ ë¹ˆë„
pair_freq = {
    ('h', 'e'): 3,    # 'h'ì™€ 'e'ê°€ ì—°ì†ìœ¼ë¡œ 3ë²ˆ ë‚˜ì˜´
    ('e', 'l'): 2,
    ('l', 'l'): 4,    # 'l'ê³¼ 'l'ì´ ì—°ì†ìœ¼ë¡œ 4ë²ˆ ë‚˜ì˜´
}

# ìš©ë„: í›ˆë ¨ ì‹œ ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ ì°¾ê¸°
max_pair = max(pair_freq, key=pair_freq.get)
# ('l', 'l') â† ë¹ˆë„ê°€ 4ë¡œ ê°€ì¥ ë†’ìŒ
```

#### ìë£Œêµ¬ì¡° ì‹œê°í™”

```
í›ˆë ¨ ê³¼ì •:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ì…ë ¥ í…ìŠ¤íŠ¸    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë¬¸ì ë¶„í•´       â”‚
â”‚  ['h','e',...]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pair Frequencies â”‚  â† ìŒ ë¹ˆë„ ê³„ì‚°
â”‚  ('l','l'): 4   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Merge Rules    â”‚  â† ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ ì €ì¥
â”‚  [('l','l')]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vocabulary    â”‚  â† ìƒˆ í† í° ì¶”ê°€
â”‚  {..., 'll': 6} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì¸ì½”ë”© ê³¼ì •:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ì…ë ¥ í…ìŠ¤íŠ¸    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Merge Rules    â”‚  â† ê·œì¹™ ìˆœì„œëŒ€ë¡œ ì ìš©
â”‚  ì ìš©           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vocabulary    â”‚  â† í† í°ì„ IDë¡œ ë³€í™˜
â”‚  ë§¤í•‘           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    [4, 5, 3]
```

#### ì‹¤ìŠµ: ìë£Œêµ¬ì¡° ë§Œë“¤ì–´ë³´ê¸°

```python
# ë‹¤ìŒ ë°ì´í„°ë¡œ ìë£Œêµ¬ì¡°ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ë³´ì„¸ìš”

text = "aa bb aa"

# 1. ì´ˆê¸° Vocabulary ë§Œë“¤ê¸°
vocab = {
    'a': 0,
    'b': 1,
    ' ': 2,
}

# 2. Pair Frequencies ê³„ì‚°í•˜ê¸°
# í† í°: ['a', 'a', ' ', 'b', 'b', ' ', 'a', 'a']
pair_freq = {
    ('a', 'a'): ?,  # ëª‡ ë²ˆ ë‚˜ì˜¤ë‚˜ìš”?
    ('a', ' '): ?,
    ('b', 'b'): ?,
    # ... ë‚˜ë¨¸ì§€ë„ ì±„ì›Œë³´ì„¸ìš”
}

# 3. ì²« ë²ˆì§¸ Merge Rule ë§Œë“¤ê¸°
# ê°€ì¥ ë¹ˆë²ˆí•œ ìŒì€?
max_pair = ?

# 4. Vocabulary ì—…ë°ì´íŠ¸
# ìƒˆë¡œìš´ í† í° ì¶”ê°€
new_token = max_pair[0] + max_pair[1]  # 'aa' ë˜ëŠ” 'bb'
vocab[new_token] = 3  # ë‹¤ìŒ ID

print("ì—…ë°ì´íŠ¸ëœ Vocabulary:", vocab)
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] Vocabulary, Merge Rules, Pair Frequenciesì˜ ì—­í• ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?
- [ ] ë”•ì…”ë„ˆë¦¬ ìë£Œêµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ì¤„ ì•„ë‚˜ìš”?
- [ ] max() í•¨ìˆ˜ë¡œ ìµœëŒ“ê°’ ì°¾ëŠ” ë°©ë²•ì„ ì•„ë‚˜ìš”?

---

## Week 3-4: ê¸°ë³¸ BPE êµ¬í˜„

### êµ¬í˜„ ìˆœì„œ
```
base.py â†’ basic.py â†’ í…ŒìŠ¤íŠ¸
```

### ğŸ“… Week 3 Day 1-3: "base.py ì‘ì„±í•˜ê¸°"

#### í•™ìŠµ ëª©í‘œ
- í† í¬ë‚˜ì´ì €ì˜ ê¸°ë³¸ í‹€(Base Class) ë§Œë“¤ê¸°
- ëª¨ë“  í† í¬ë‚˜ì´ì €ê°€ ê°€ì ¸ì•¼ í•  ê³µí†µ ê¸°ëŠ¥ ì •ì˜í•˜ê¸°

#### base.py íŒŒì¼ êµ¬ì¡°

```python
# bpe/base.py

class Tokenizer:
    """
    ëª¨ë“  í† í¬ë‚˜ì´ì €ì˜ ë¶€ëª¨ í´ë˜ìŠ¤
    """

    def __init__(self):
        # ì—¬ê¸°ì„œ ì´ˆê¸°í™”í•  ê²ƒë“¤:
        # - Vocabulary
        # - Merge Rules
        pass

    def train(self, text, vocab_size):
        """
        í…ìŠ¤íŠ¸ë¡œ í† í¬ë‚˜ì´ì € í›ˆë ¨í•˜ê¸°

        Args:
            text: í›ˆë ¨í•  í…ìŠ¤íŠ¸
            vocab_size: ìµœì¢… ì–´íœ˜ í¬ê¸°
        """
        raise NotImplementedError

    def encode(self, text):
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ë³€í™˜

        Args:
            text: ì¸ì½”ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            list: í† í° ID ë¦¬ìŠ¤íŠ¸
        """
        raise NotImplementedError

    def decode(self, ids):
        """
        í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            ids: í† í° ID ë¦¬ìŠ¤íŠ¸

        Returns:
            str: ë³µì›ëœ í…ìŠ¤íŠ¸
        """
        raise NotImplementedError

    def save(self, filepath):
        """
        í† í¬ë‚˜ì´ì €ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        """
        raise NotImplementedError

    def load(self, filepath):
        """
        íŒŒì¼ì—ì„œ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
        """
        raise NotImplementedError
```

#### ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ

**Step 1: í”„ë¡œì íŠ¸ êµ¬ì¡° ë§Œë“¤ê¸°**
```bash
# í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
cd C:\Users\namya\OneDrive\Desktop\Project\custom_BPE
mkdir bpe
cd bpe
type nul > __init__.py
type nul > base.py
```

**Step 2: base.py ì‘ì„±í•˜ê¸°**

```python
# bpe/base.py
# ì²œì²œíˆ ë”°ë¼ íƒ€ì´í•‘í•˜ì„¸ìš”!

class Tokenizer:
    """
    ë² ì´ìŠ¤ í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤

    ëª¨ë“  í† í¬ë‚˜ì´ì €ê°€ ìƒì†ë°›ì„ ë¶€ëª¨ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ê³µí†µ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ì•„ì§ êµ¬í˜„ ì•ˆ í•¨ (ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        pass

    def train(self, text, vocab_size):
        """
        í† í¬ë‚˜ì´ì € í›ˆë ¨

        ìì‹ í´ë˜ìŠ¤ì—ì„œ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        raise NotImplementedError("train ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤")

    def encode(self, text):
        """
        í…ìŠ¤íŠ¸ â†’ í† í° ID

        ìì‹ í´ë˜ìŠ¤ì—ì„œ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        raise NotImplementedError("encode ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤")

    def decode(self, ids):
        """
        í† í° ID â†’ í…ìŠ¤íŠ¸

        ìì‹ í´ë˜ìŠ¤ì—ì„œ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        raise NotImplementedError("decode ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤")

    def save(self, filepath):
        """ëª¨ë¸ ì €ì¥ (ì„ íƒì  êµ¬í˜„)"""
        raise NotImplementedError("save ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤")

    def load(self, filepath):
        """ëª¨ë¸ ë¡œë“œ (ì„ íƒì  êµ¬í˜„)"""
        raise NotImplementedError("load ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤")
```

**Step 3: í…ŒìŠ¤íŠ¸í•´ë³´ê¸°**

```python
# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ (í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰)
# bpe í´ë” ë°–ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”!

from bpe.base import Tokenizer

# ê°ì²´ ìƒì„±
tokenizer = Tokenizer()

# ë©”ì„œë“œ í˜¸ì¶œí•´ë³´ê¸° (ì—ëŸ¬ê°€ ë‚˜ì•¼ ì •ìƒ!)
try:
    tokenizer.train("test", 100)
except NotImplementedError as e:
    print("ì˜ˆìƒëœ ì—ëŸ¬:", e)
    # ì¶œë ¥: ì˜ˆìƒëœ ì—ëŸ¬: train ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤
```

#### ê°œë… ì´í•´: ì™œ Base Classê°€ í•„ìš”í•œê°€?

**ë¹„ìœ : ìë™ì°¨ ì„¤ê³„ë„**
```
Base Class (Tokenizer)
    = ìë™ì°¨ì˜ ê¸°ë³¸ ì„¤ê³„ë„
    = "ëª¨ë“  ìë™ì°¨ëŠ” ì´ëŸ° ê¸°ëŠ¥ì´ ìˆì–´ì•¼ í•´!"

Child Class (BasicTokenizer)
    = ì‹¤ì œ ìë™ì°¨
    = "ì„¤ê³„ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§„ì§œ ìë™ì°¨ ë§Œë“¤ê¸°"
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] bpe í´ë”ì™€ íŒŒì¼ë“¤ì„ ë§Œë“¤ì—ˆë‚˜ìš”?
- [ ] base.py ì½”ë“œë¥¼ ì§ì ‘ íƒ€ì´í•‘í–ˆë‚˜ìš”?
- [ ] NotImplementedErrorì˜ ì—­í• ì„ ì´í•´í–ˆë‚˜ìš”?
- [ ] í´ë˜ìŠ¤ ìƒì† ê°œë…ì„ ì•Œê³  ìˆë‚˜ìš”?

---

### ğŸ“… Week 3 Day 4-7: "basic.py - ì´ˆê¸°í™”ì™€ í—¬í¼ í•¨ìˆ˜"

#### í•™ìŠµ ëª©í‘œ
- BasicTokenizer í´ë˜ìŠ¤ ë¼ˆëŒ€ ë§Œë“¤ê¸°
- í—¬í¼ í•¨ìˆ˜ êµ¬í˜„í•˜ê¸°

#### basic.py íŒŒì¼ êµ¬ì¡° ë¯¸ë¦¬ë³´ê¸°

```python
# bpe/basic.py

from .base import Tokenizer

class BasicTokenizer(Tokenizer):
    """ê¸°ë³¸ BPE í† í¬ë‚˜ì´ì €"""

    def __init__(self):
        # 1. ì´ˆê¸°í™”
        pass

    def _get_stats(self, tokens):
        # 2. ìŒ ë¹ˆë„ ê³„ì‚° (í—¬í¼ í•¨ìˆ˜)
        pass

    def _merge(self, tokens, pair):
        # 3. ìŒ ë³‘í•© (í—¬í¼ í•¨ìˆ˜)
        pass

    def train(self, text, vocab_size):
        # 4. í›ˆë ¨ (ë©”ì¸ í•¨ìˆ˜)
        pass

    def encode(self, text):
        # 5. ì¸ì½”ë”©
        pass

    def decode(self, ids):
        # 6. ë””ì½”ë”©
        pass
```

#### Step 1: __init__ êµ¬í˜„

```python
# bpe/basic.py

from .base import Tokenizer

class BasicTokenizer(Tokenizer):
    """
    ê¸°ë³¸ BPE í† í¬ë‚˜ì´ì €

    ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì˜ BPEë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """
        ì´ˆê¸°í™” ë©”ì„œë“œ

        í•„ìš”í•œ ìë£Œêµ¬ì¡°:
        1. merges: ë³‘í•© ê·œì¹™ ë¦¬ìŠ¤íŠ¸
        2. vocab: ì–´íœ˜ ì‚¬ì „ (í† í° â†’ ID)
        """
        super().__init__()  # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”

        # ë³‘í•© ê·œì¹™ ì €ì¥ì†Œ
        # ì˜ˆ: [('h', 'e'), ('l', 'l'), ...]
        self.merges = []

        # ì–´íœ˜ ì‚¬ì „
        # ì˜ˆ: {'h': 0, 'e': 1, 'he': 2, ...}
        self.vocab = {}
```

**ê°œë… ì´í•´:**
```python
# self.mergesëŠ” ì™œ ë¦¬ìŠ¤íŠ¸ì¸ê°€ìš”?
# â†’ ìˆœì„œê°€ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸! (Rule 1, Rule 2, ...)

# self.vocabì€ ì™œ ë”•ì…”ë„ˆë¦¬ì¸ê°€ìš”?
# â†’ ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´! O(1) ì‹œê°„ ë³µì¡ë„
```

#### Step 2: _get_stats êµ¬í˜„ (ìŒ ë¹ˆë„ ê³„ì‚°)

```python
def _get_stats(self, tokens):
    """
    í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì—°ì†ëœ ìŒì˜ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        tokens: í† í° ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['h', 'e', 'l', 'l', 'o'])

    Returns:
        dict: ìŒ â†’ ë¹ˆë„ (ì˜ˆ: {('h', 'e'): 1, ('l', 'l'): 1, ...})

    ì˜ˆì‹œ:
        tokens = ['a', 'b', 'a', 'b']
        ê²°ê³¼ = {('a', 'b'): 2, ('b', 'a'): 1}
    """
    # ë¹ˆë„ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    counts = {}

    # ëª¨ë“  ì—°ì†ëœ ìŒì„ í™•ì¸
    for i in range(len(tokens) - 1):
        # í˜„ì¬ í† í°ê³¼ ë‹¤ìŒ í† í°ì˜ ìŒ
        pair = (tokens[i], tokens[i + 1])

        # ë”•ì…”ë„ˆë¦¬ì— ë¹ˆë„ ì €ì¥
        # get(pair, 0): pairê°€ ì—†ìœ¼ë©´ 0 ë°˜í™˜
        counts[pair] = counts.get(pair, 0) + 1

    return counts
```

**ì†ìœ¼ë¡œ ë”°ë¼í•˜ê¸°:**
```python
# ì˜ˆì‹œ: tokens = ['h', 'e', 'l', 'l', 'o']

# i = 0: pair = ('h', 'e')
#   counts = {('h', 'e'): 1}

# i = 1: pair = ('e', 'l')
#   counts = {('h', 'e'): 1, ('e', 'l'): 1}

# i = 2: pair = ('l', 'l')
#   counts = {('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1}

# i = 3: pair = ('l', 'o')
#   counts = {('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1}

# i = 4: ë²”ìœ„ ë²—ì–´ë‚¨ (ì¢…ë£Œ)
```

**ì‹¤ìŠµ ê³¼ì œ:**
```python
# ì§ì ‘ ì½”ë“œë¥¼ ì‘ì„±í•´ì„œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”

def _get_stats(tokens):
    counts = {}
    for i in range(len(tokens) - 1):
        pair = (tokens[i], tokens[i + 1])
        counts[pair] = counts.get(pair, 0) + 1
    return counts

# í…ŒìŠ¤íŠ¸
test_tokens = ['a', 'b', 'a', 'b', 'c']
result = _get_stats(test_tokens)
print(result)

# ì˜ˆìƒ ê²°ê³¼:
# {('a', 'b'): 2, ('b', 'a'): 1, ('b', 'c'): 1}
```

#### Step 3: _merge êµ¬í˜„ (ìŒ ë³‘í•©)

```python
def _merge(self, tokens, pair):
    """
    í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ìŒì„ ì°¾ì•„ ë³‘í•©í•©ë‹ˆë‹¤.

    Args:
        tokens: í† í° ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['h', 'e', 'l', 'l', 'o'])
        pair: ë³‘í•©í•  ìŒ (ì˜ˆ: ('l', 'l'))

    Returns:
        list: ë³‘í•©ëœ ìƒˆ í† í° ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['h', 'e', 'll', 'o'])

    ì˜ˆì‹œ:
        tokens = ['a', 'b', 'a', 'b']
        pair = ('a', 'b')
        ê²°ê³¼ = ['ab', 'ab']
    """
    new_tokens = []  # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    i = 0  # í˜„ì¬ ìœ„ì¹˜

    while i < len(tokens):
        # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ìŒì´ ë°œê²¬ë˜ëŠ”ì§€ í™•ì¸
        if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:
            # ìŒì„ ë³‘í•©í•˜ì—¬ ì¶”ê°€
            new_tokens.append(tokens[i] + tokens[i + 1])
            i += 2  # ë‘ ì¹¸ ê±´ë„ˆë›°ê¸°
        else:
            # ìŒì´ ì•„ë‹ˆë©´ ê·¸ëƒ¥ ì¶”ê°€
            new_tokens.append(tokens[i])
            i += 1  # í•œ ì¹¸ ì´ë™

    return new_tokens
```

**ì†ìœ¼ë¡œ ë”°ë¼í•˜ê¸°:**
```python
# ì˜ˆì‹œ: tokens = ['h', 'e', 'l', 'l', 'o'], pair = ('l', 'l')

# i = 0:
#   tokens[0:2] = ('h', 'e') â‰  ('l', 'l')
#   new_tokens = ['h'], i = 1

# i = 1:
#   tokens[1:3] = ('e', 'l') â‰  ('l', 'l')
#   new_tokens = ['h', 'e'], i = 2

# i = 2:
#   tokens[2:4] = ('l', 'l') == ('l', 'l')  â† ë°œê²¬!
#   new_tokens = ['h', 'e', 'll'], i = 4

# i = 4:
#   new_tokens = ['h', 'e', 'll', 'o'], i = 5

# ê²°ê³¼: ['h', 'e', 'll', 'o']
```

**ì‹¤ìŠµ ê³¼ì œ:**
```python
# ì§ì ‘ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”

def _merge(tokens, pair):
    new_tokens = []
    i = 0
    while i < len(tokens):
        if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:
            new_tokens.append(tokens[i] + tokens[i + 1])
            i += 2
        else:
            new_tokens.append(tokens[i])
            i += 1
    return new_tokens

# í…ŒìŠ¤íŠ¸ 1
tokens = ['a', 'b', 'a', 'b']
result = _merge(tokens, ('a', 'b'))
print(result)  # ['ab', 'ab']

# í…ŒìŠ¤íŠ¸ 2
tokens = ['h', 'e', 'l', 'l', 'o']
result = _merge(tokens, ('l', 'l'))
print(result)  # ['h', 'e', 'll', 'o']
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] BasicTokenizer í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì—ˆë‚˜ìš”?
- [ ] _get_stats í•¨ìˆ˜ë¥¼ ì´í•´í•˜ê³  êµ¬í˜„í–ˆë‚˜ìš”?
- [ ] _merge í•¨ìˆ˜ë¥¼ ì´í•´í•˜ê³  êµ¬í˜„í–ˆë‚˜ìš”?
- [ ] ê° í•¨ìˆ˜ë¥¼ ì†ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•´ë´¤ë‚˜ìš”?

---

### ğŸ“… Week 4 Day 1-4: "basic.py - train ë©”ì„œë“œ êµ¬í˜„"

#### í•™ìŠµ ëª©í‘œ
- BPE í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜ ì™„ì„±í•˜ê¸°
- ì „ì²´ íë¦„ ì´í•´í•˜ê¸°

#### train ë©”ì„œë“œ ì „ì²´ êµ¬ì¡°

```python
def train(self, text, vocab_size):
    """
    BPE í† í¬ë‚˜ì´ì € í›ˆë ¨

    Args:
        text: í›ˆë ¨ í…ìŠ¤íŠ¸ (ì˜ˆ: "hello world")
        vocab_size: ëª©í‘œ ì–´íœ˜ í¬ê¸° (ì˜ˆ: 300)

    ê³¼ì •:
    1. í…ìŠ¤íŠ¸ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
    2. ì´ˆê¸° ì–´íœ˜ ìƒì„± (256ê°œ ë°”ì´íŠ¸)
    3. ë°˜ë³µì ìœ¼ë¡œ ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ ë³‘í•©
    4. vocab_sizeì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ
    """
```

#### Step 1: í…ìŠ¤íŠ¸ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜

```python
def train(self, text, vocab_size):
    """BPE í›ˆë ¨"""

    # === STEP 1: í…ìŠ¤íŠ¸ â†’ ë°”ì´íŠ¸ ===
    # ì™œ ë°”ì´íŠ¸?
    # - ëª¨ë“  í…ìŠ¤íŠ¸ (í•œê¸€, ì˜ì–´, ì´ëª¨ì§€ ë“±)ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    # - 256ê°œì˜ ê°€ëŠ¥í•œ ê°’ (0-255)

    # í…ìŠ¤íŠ¸ë¥¼ ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©
    text_bytes = text.encode('utf-8')

    # ë°”ì´íŠ¸ë¥¼ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    tokens = list(text_bytes)

    # ì˜ˆì‹œ:
    # text = "hi"
    # text_bytes = b'hi'
    # tokens = [104, 105]  # 'h'=104, 'i'=105 (ASCII ì½”ë“œ)

    print(f"ì´ˆê¸° í† í° ìˆ˜: {len(tokens)}")
```

**ê°œë… ì´í•´: ë°”ì´íŠ¸ ì¸ì½”ë”©**
```python
# ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°

text = "ì•ˆë…•"
text_bytes = text.encode('utf-8')
print(text_bytes)  # b'\xec\x95\x88\xeb\x85\x95'

tokens = list(text_bytes)
print(tokens)  # [236, 149, 136, 235, 133, 149]

# í•œê¸€ "ì•ˆë…•" â†’ 6ê°œì˜ ë°”ì´íŠ¸ë¡œ í‘œí˜„ë¨!
```

#### Step 2: ì´ˆê¸° ì–´íœ˜ ìƒì„±

```python
def train(self, text, vocab_size):
    # ... (ì´ì „ ì½”ë“œ)

    # === STEP 2: ì´ˆê¸° ì–´íœ˜ ìƒì„± ===
    # 0-255ì˜ ëª¨ë“  ë°”ì´íŠ¸ ê°’ì„ ì–´íœ˜ì— ì¶”ê°€

    self.vocab = {}
    for i in range(256):
        # ë°”ì´íŠ¸ ê°’ â†’ ë°”ì´íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜
        # ì˜ˆ: 104 â†’ b'h'
        self.vocab[bytes([i])] = i

    # í˜„ì¬ ì–´íœ˜ í¬ê¸° = 256
    print(f"ì´ˆê¸° ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")

    # ì˜ˆì‹œ:
    # vocab = {
    #     b'\x00': 0,
    #     b'\x01': 1,
    #     ...
    #     b'h': 104,
    #     b'i': 105,
    #     ...
    #     b'\xff': 255
    # }
```

#### Step 3: ë³‘í•© ë°˜ë³µ

```python
def train(self, text, vocab_size):
    # ... (ì´ì „ ì½”ë“œ)

    # === STEP 3: ë³‘í•© ë°˜ë³µ ===
    # vocab_sizeì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ

    num_merges = vocab_size - 256
    # ì˜ˆ: vocab_size=300ì´ë©´, 44ë²ˆ ë³‘í•©

    for merge_idx in range(num_merges):
        # 3-1. í˜„ì¬ í† í°ì—ì„œ ìŒ ë¹ˆë„ ê³„ì‚°
        stats = self._get_stats(tokens)

        # ë¹ˆë„ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ (ë” ì´ìƒ ë³‘í•©í•  ìŒì´ ì—†ìŒ)
        if not stats:
            break

        # 3-2. ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ ì°¾ê¸°
        max_pair = max(stats, key=stats.get)
        # max_pair = (token1, token2)

        print(f"ë³‘í•© {merge_idx + 1}: {max_pair} (ë¹ˆë„: {stats[max_pair]})")

        # 3-3. ë³‘í•© ê·œì¹™ ì €ì¥
        self.merges.append(max_pair)

        # 3-4. ìƒˆ í† í°ì„ ì–´íœ˜ì— ì¶”ê°€
        new_token = bytes([max_pair[0]]) + bytes([max_pair[1]])
        new_id = 256 + merge_idx
        self.vocab[new_token] = new_id

        # 3-5. í† í° ë¦¬ìŠ¤íŠ¸ì— ë³‘í•© ì ìš©
        tokens = self._merge(tokens, max_pair)

        print(f"í˜„ì¬ í† í° ìˆ˜: {len(tokens)}")

    print(f"í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")
```

**ì „ì²´ train ë©”ì„œë“œ (ì™„ì„±ë³¸)**

```python
def train(self, text, vocab_size):
    """
    BPE í† í¬ë‚˜ì´ì € í›ˆë ¨

    Args:
        text (str): í›ˆë ¨ í…ìŠ¤íŠ¸
        vocab_size (int): ëª©í‘œ ì–´íœ˜ í¬ê¸°
    """
    # Step 1: í…ìŠ¤íŠ¸ â†’ ë°”ì´íŠ¸ â†’ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸
    text_bytes = text.encode('utf-8')
    tokens = list(text_bytes)

    # Step 2: ì´ˆê¸° ì–´íœ˜ ìƒì„± (0-255)
    self.vocab = {bytes([i]): i for i in range(256)}

    # Step 3: ë³‘í•© ë°˜ë³µ
    num_merges = vocab_size - 256

    for merge_idx in range(num_merges):
        # ìŒ ë¹ˆë„ ê³„ì‚°
        stats = self._get_stats(tokens)
        if not stats:
            break

        # ìµœë¹ˆ ìŒ ì°¾ê¸°
        max_pair = max(stats, key=stats.get)

        # ë³‘í•© ê·œì¹™ ì €ì¥
        self.merges.append(max_pair)

        # ì–´íœ˜ ì—…ë°ì´íŠ¸
        new_token = bytes([max_pair[0]]) + bytes([max_pair[1]])
        self.vocab[new_token] = 256 + merge_idx

        # ë³‘í•© ì ìš©
        tokens = self._merge(tokens, max_pair)

    print(f"í›ˆë ¨ ì™„ë£Œ! ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")
```

#### ì‹¤ìŠµ: ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ í›ˆë ¨í•´ë³´ê¸°

```python
# í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±

from bpe.basic import BasicTokenizer

# í† í¬ë‚˜ì´ì € ìƒì„±
tokenizer = BasicTokenizer()

# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ í›ˆë ¨
text = "aaabbb aaabbb"
tokenizer.train(text, vocab_size=260)  # 256 + 4ë²ˆ ë³‘í•©

# ì¶œë ¥ ì˜ˆì‹œ:
# ì´ˆê¸° í† í° ìˆ˜: 13
# ë³‘í•© 1: (97, 97) ë¹ˆë„: 6
# ë³‘í•© 2: (98, 98) ë¹ˆë„: 6
# ...
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] ì™œ í…ìŠ¤íŠ¸ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜í•˜ëŠ”ì§€ ì´í•´í–ˆë‚˜ìš”?
- [ ] ì´ˆê¸° ì–´íœ˜ í¬ê¸°ê°€ ì™œ 256ì¸ì§€ ì•„ë‚˜ìš”?
- [ ] train ë©”ì„œë“œì˜ ì „ì²´ íë¦„ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?
- [ ] ì‹¤ì œë¡œ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  í…ŒìŠ¤íŠ¸í•´ë´¤ë‚˜ìš”?

---

### ğŸ“… Week 4 Day 5-7: "basic.py - encode/decode êµ¬í˜„"

#### í•™ìŠµ ëª©í‘œ
- ì¸ì½”ë”© ë©”ì„œë“œ êµ¬í˜„í•˜ê¸°
- ë””ì½”ë”© ë©”ì„œë“œ êµ¬í˜„í•˜ê¸°
- ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸í•˜ê¸°

#### encode ë©”ì„œë“œ êµ¬í˜„

```python
def encode(self, text):
    """
    í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ì¸ì½”ë”©

    Args:
        text (str): ì¸ì½”ë”©í•  í…ìŠ¤íŠ¸

    Returns:
        list: í† í° ID ë¦¬ìŠ¤íŠ¸

    ì•Œê³ ë¦¬ì¦˜:
    1. í…ìŠ¤íŠ¸ â†’ ë°”ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
    2. ë³‘í•© ê·œì¹™ì„ ìˆœì„œëŒ€ë¡œ ì ìš©
    3. í† í° â†’ ID ë³€í™˜
    """
    # Step 1: í…ìŠ¤íŠ¸ â†’ ë°”ì´íŠ¸
    text_bytes = text.encode('utf-8')
    tokens = list(text_bytes)

    # Step 2: ë³‘í•© ê·œì¹™ ì ìš© (í›ˆë ¨ ì‹œ ì €ì¥í•œ ìˆœì„œëŒ€ë¡œ)
    for pair in self.merges:
        tokens = self._merge(tokens, pair)

    # Step 3: í† í° â†’ ID
    # ê° í† í°ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„ vocabì—ì„œ ID ì°¾ê¸°
    ids = []
    for token in tokens:
        # tokenì´ ì •ìˆ˜ë©´ â†’ bytes([token])
        if isinstance(token, int):
            token_bytes = bytes([token])
        else:
            token_bytes = token

        # vocabì—ì„œ ID ì°¾ê¸°
        ids.append(self.vocab[token_bytes])

    return ids
```

**ê°œë… ì´í•´: ì™œ ë³µì¡í•œê°€ìš”?**

```python
# ë¬¸ì œ: tokensì—ëŠ” ì •ìˆ˜ì™€ ë°”ì´íŠ¸ê°€ ì„ì—¬ìˆì„ ìˆ˜ ìˆìŒ

# í›ˆë ¨ ì „: tokens = [104, 105]  # ëª¨ë‘ ì •ìˆ˜
# ë³‘í•© í›„: tokens = [b'hi']     # ë°”ì´íŠ¸ ë¬¸ìì—´

# ë”°ë¼ì„œ íƒ€ì… í™•ì¸ì´ í•„ìš”!
if isinstance(token, int):
    token_bytes = bytes([token])  # ì •ìˆ˜ â†’ ë°”ì´íŠ¸
else:
    token_bytes = token  # ì´ë¯¸ ë°”ì´íŠ¸
```

**ê°œì„ ëœ encode (ë” ê¹”ë”í•œ ë²„ì „)**

```python
def encode(self, text):
    """í…ìŠ¤íŠ¸ â†’ í† í° ID"""
    # í…ìŠ¤íŠ¸ â†’ ë°”ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
    tokens = list(text.encode('utf-8'))

    # ë³‘í•© ê·œì¹™ ì ìš©
    for pair in self.merges:
        tokens = self._merge(tokens, pair)

    # í† í° â†’ ID
    ids = []
    for token in tokens:
        # ì •ìˆ˜ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
        token_bytes = bytes([token]) if isinstance(token, int) else token.encode() if isinstance(token, str) else token
        ids.append(self.vocab[token_bytes])

    return ids
```

#### decode ë©”ì„œë“œ êµ¬í˜„

```python
def decode(self, ids):
    """
    í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©

    Args:
        ids (list): í† í° ID ë¦¬ìŠ¤íŠ¸

    Returns:
        str: ë³µì›ëœ í…ìŠ¤íŠ¸

    ì•Œê³ ë¦¬ì¦˜:
    1. ID â†’ í† í° (ë°”ì´íŠ¸)
    2. ëª¨ë“  í† í° ì—°ê²°
    3. ë°”ì´íŠ¸ â†’ í…ìŠ¤íŠ¸
    """
    # Step 1: ì—­ë°©í–¥ ì–´íœ˜ ìƒì„± (ID â†’ í† í°)
    # vocab = {í† í°: ID} â†’ reverse = {ID: í† í°}
    reverse_vocab = {v: k for k, v in self.vocab.items()}

    # Step 2: ID â†’ í† í° (ë°”ì´íŠ¸)
    tokens = [reverse_vocab[id] for id in ids]

    # Step 3: í† í° ì—°ê²°
    text_bytes = b''.join(tokens)

    # Step 4: ë°”ì´íŠ¸ â†’ í…ìŠ¤íŠ¸
    text = text_bytes.decode('utf-8')

    return text
```

**ë” ê°„ë‹¨í•œ ë²„ì „:**

```python
def decode(self, ids):
    """í† í° ID â†’ í…ìŠ¤íŠ¸"""
    # ID â†’ í† í° ë§¤í•‘ ìƒì„±
    id_to_token = {v: k for k, v in self.vocab.items()}

    # IDë“¤ì„ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì—°ê²°
    text_bytes = b''.join(id_to_token[id] for id in ids)

    # ë°”ì´íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë””ì½”ë”©
    return text_bytes.decode('utf-8')
```

#### ì „ì²´ í…ŒìŠ¤íŠ¸

```python
# ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ

from bpe.basic import BasicTokenizer

# 1. í† í¬ë‚˜ì´ì € ìƒì„±
tokenizer = BasicTokenizer()

# 2. í›ˆë ¨
train_text = """
This is a sample text for training BPE tokenizer.
BPE stands for Byte Pair Encoding.
"""
tokenizer.train(train_text, vocab_size=300)

# 3. ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
test_text = "This is BPE"
ids = tokenizer.encode(test_text)
print(f"ì¸ì½”ë”© ê²°ê³¼: {ids}")

# 4. ë””ì½”ë”© í…ŒìŠ¤íŠ¸
decoded_text = tokenizer.decode(ids)
print(f"ë””ì½”ë”© ê²°ê³¼: {decoded_text}")

# 5. ì™•ë³µ í…ŒìŠ¤íŠ¸
assert test_text == decoded_text, "ë””ì½”ë”©ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤!"
print("ì„±ê³µ! ì¸ì½”ë”©ê³¼ ë””ì½”ë”©ì´ ì •í™•í•©ë‹ˆë‹¤.")
```

#### ë””ë²„ê¹… íŒ

```python
# ë¬¸ì œê°€ ìƒê¸°ë©´ ì´ë ‡ê²Œ í™•ì¸í•˜ì„¸ìš”

# 1. ë³‘í•© ê·œì¹™ í™•ì¸
print(f"ë³‘í•© ê·œì¹™ ê°œìˆ˜: {len(tokenizer.merges)}")
print(f"ì²« 5ê°œ ê·œì¹™: {tokenizer.merges[:5]}")

# 2. ì–´íœ˜ í¬ê¸° í™•ì¸
print(f"ì–´íœ˜ í¬ê¸°: {len(tokenizer.vocab)}")

# 3. ì¤‘ê°„ ê³¼ì • ì¶œë ¥
def encode_debug(self, text):
    tokens = list(text.encode('utf-8'))
    print(f"ì´ˆê¸°: {tokens}")

    for i, pair in enumerate(self.merges):
        tokens = self._merge(tokens, pair)
        print(f"ë³‘í•© {i+1} í›„: {tokens}")

    return tokens
```

#### ì²´í¬ í¬ì¸íŠ¸
- [ ] encode ë©”ì„œë“œë¥¼ êµ¬í˜„í–ˆë‚˜ìš”?
- [ ] decode ë©”ì„œë“œë¥¼ êµ¬í˜„í–ˆë‚˜ìš”?
- [ ] ì™•ë³µ í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•˜ë‚˜ìš”?
- [ ] ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë””ë²„ê¹… ë°©ë²•ì„ ì•Œê³  ìˆë‚˜ìš”?

---

## Week 5: ê³ ê¸‰ ê¸°ëŠ¥ êµ¬í˜„

### ğŸ“… Day 1-3: "ëª¨ë¸ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°"

#### í•™ìŠµ ëª©í‘œ
- í›ˆë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°
- ì €ì¥ëœ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
- JSON í˜•ì‹ ì´í•´í•˜ê¸°

#### ì™œ ì €ì¥ì´ í•„ìš”í•œê°€ìš”?

```
ë¬¸ì œ:
- í›ˆë ¨ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ (í° ë°ì´í„°ì…‹ì˜ ê²½ìš°)
- ë§¤ë²ˆ í›ˆë ¨í•˜ë©´ ë¹„íš¨ìœ¨ì 

í•´ê²°:
- í•œ ë²ˆ í›ˆë ¨í•˜ê³  ì €ì¥
- í•„ìš”í•  ë•Œ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©
```

#### save ë©”ì„œë“œ êµ¬í˜„

```python
import json

def save(self, filepath):
    """
    í† í¬ë‚˜ì´ì €ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥

    Args:
        filepath (str): ì €ì¥í•  íŒŒì¼ ê²½ë¡œ

    ì €ì¥í•  ì •ë³´:
    1. merges: ë³‘í•© ê·œì¹™
    2. vocab: ì–´íœ˜ ì‚¬ì „
    """
    # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
    save_data = {
        'merges': self.merges,  # ë¦¬ìŠ¤íŠ¸
        'vocab': {
            # ë°”ì´íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (JSONì€ ë°”ì´íŠ¸ë¥¼ ì§ì ‘ ì €ì¥ ëª»í•¨)
            k.decode('latin-1'): v
            for k, v in self.vocab.items()
        }
    }

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False)

    print(f"í† í¬ë‚˜ì´ì €ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

#### load ë©”ì„œë“œ êµ¬í˜„

```python
def load(self, filepath):
    """
    JSON íŒŒì¼ì—ì„œ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°

    Args:
        filepath (str): ë¶ˆëŸ¬ì˜¬ íŒŒì¼ ê²½ë¡œ
    """
    # JSON íŒŒì¼ ì½ê¸°
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ë³‘í•© ê·œì¹™ ë³µì›
    self.merges = [tuple(pair) for pair in data['merges']]

    # ì–´íœ˜ ë³µì› (ë¬¸ìì—´ â†’ ë°”ì´íŠ¸)
    self.vocab = {
        k.encode('latin-1'): v
        for k, v in data['vocab'].items()
    }

    print(f"{filepath}ì—ì„œ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
```

#### ì‚¬ìš© ì˜ˆì‹œ

```python
# 1. í›ˆë ¨ í›„ ì €ì¥
tokenizer = BasicTokenizer()
tokenizer.train(text, vocab_size=300)
tokenizer.save('models/my_tokenizer.json')

# 2. ë‚˜ì¤‘ì— ë¶ˆëŸ¬ì˜¤ê¸°
new_tokenizer = BasicTokenizer()
new_tokenizer.load('models/my_tokenizer.json')

# 3. ë°”ë¡œ ì‚¬ìš©
ids = new_tokenizer.encode("Hello")
```

---

### ğŸ“… Day 4-7: "RegexBPE êµ¬í˜„ (ì„ íƒì‚¬í•­)"

#### í•™ìŠµ ëª©í‘œ
- GPT ìŠ¤íƒ€ì¼ í† í¬ë‚˜ì´ì € ì´í•´í•˜ê¸°
- ì •ê·œì‹ íŒ¨í„´ í•™ìŠµí•˜ê¸°

#### RegexBPEê°€ ë­”ê°€ìš”?

```
BasicBPE:
- í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ ì²˜ë¦¬
- ë‹¨ì–´ ê²½ê³„ ê³ ë ¤ ì•ˆ í•¨

RegexBPE (GPT ìŠ¤íƒ€ì¼):
- ë¨¼ì € ì •ê·œì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
- ê° ì¡°ê°ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
- ë” ë‚˜ì€ ì„±ëŠ¥!
```

#### ê°„ë‹¨í•œ êµ¬í˜„ ì˜ˆì‹œ

```python
import regex as re
from .basic import BasicTokenizer

class RegexTokenizer(BasicTokenizer):
    """
    ì •ê·œì‹ ê¸°ë°˜ BPE (GPT-4 ìŠ¤íƒ€ì¼)
    """

    def __init__(self):
        super().__init__()

        # GPT-4 íŒ¨í„´ (ê°„ëµ ë²„ì „)
        self.pattern = re.compile(
            r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+""",
            re.IGNORECASE
        )

    def train(self, text, vocab_size):
        """ì •ê·œì‹ìœ¼ë¡œ ë¨¼ì € ë¶„í•  í›„ í›ˆë ¨"""
        # íŒ¨í„´ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
        chunks = re.findall(self.pattern, text)

        # ê° ì¡°ê°ì„ ê°œë³„ ì²˜ë¦¬
        # (êµ¬í˜„ì€ BasicTokenizerì™€ ìœ ì‚¬)
        ...
```

---

## ì‹¤ìŠµ ì˜ˆì œ

### ì˜ˆì œ 1: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ ì‹œì‘

```python
text = "low low low lower lowest"
tokenizer = BasicTokenizer()
tokenizer.train(text, vocab_size=280)

# ì–´ë–¤ ë³‘í•©ì´ ì¼ì–´ë‚ ê¹Œìš”?
# ì˜ˆìƒí•´ë³´ì„¸ìš”!
```

### ì˜ˆì œ 2: í•œê¸€ í…ìŠ¤íŠ¸

```python
text = "ì•ˆë…•í•˜ì„¸ìš” ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤"
tokenizer = BasicTokenizer()
tokenizer.train(text, vocab_size=300)

# í•œê¸€ë„ ì˜ ì²˜ë¦¬ë˜ë‚˜ìš”?
```

### ì˜ˆì œ 3: í° í…ìŠ¤íŠ¸ íŒŒì¼

```python
# ì±… í•œ ê¶Œ ë¶„ëŸ‰ì˜ í…ìŠ¤íŠ¸ë¡œ í›ˆë ¨
with open('data/book.txt', 'r', encoding='utf-8') as f:
    text = f.read()

tokenizer = BasicTokenizer()
tokenizer.train(text, vocab_size=5000)
tokenizer.save('models/book_tokenizer.json')
```

---

## ì°¸ê³  ìë£Œ

### í•„ìˆ˜ ì½ê¸° ìë£Œ

1. **Andrej Karpathyì˜ minbpe**
   - GitHub: https://github.com/karpathy/minbpe
   - YouTube: "Let's build GPT tokenizer"
   - ì„¤ëª…: ê°€ì¥ ê¹”ë”í•œ BPE êµ¬í˜„

2. **ì›ë³¸ ë…¼ë¬¸**
   - "Neural Machine Translation of Rare Words with Subword Units"
   - https://arxiv.org/abs/1508.07909
   - ì„¤ëª…: BPEë¥¼ NLPì— ì²˜ìŒ ì ìš©í•œ ë…¼ë¬¸

3. **GPT-2 ë…¼ë¬¸**
   - "Language Models are Unsupervised Multitask Learners"
   - OpenAI ë¸”ë¡œê·¸
   - ì„¤ëª…: BPEë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ì‚¬ë¡€

### ì¶”ê°€ í•™ìŠµ ìë£Œ

4. **Hugging Face Tokenizers**
   - https://huggingface.co/docs/tokenizers
   - ì„¤ëª…: ì‚°ì—… í‘œì¤€ í† í¬ë‚˜ì´ì € ë¼ì´ë¸ŒëŸ¬ë¦¬

5. **YouTube ê°•ì˜**
   - Andrej Karpathy "Let's build the GPT Tokenizer"
   - ì„¤ëª…: ì‹œê°ì ìœ¼ë¡œ BPE í•™ìŠµ

6. **ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸**
   - "Byte Pair Encoding â€” The Dark Horse of NLP"
   - Medium, Towards Data Science
   - ì„¤ëª…: ì´ˆë³´ì ì¹œí™”ì ì¸ ì„¤ëª…

### ì—°ìŠµ ë¬¸ì œ

7. **LeetCode/ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ**
   - ë¬¸ìì—´ ì²˜ë¦¬ ë¬¸ì œ
   - ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© ë¬¸ì œ
   - ì„¤ëª…: BPEì— í•„ìš”í•œ ê¸°ì´ˆ ë‹¤ì§€ê¸°

---

## í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1: ê¸°ì´ˆ ê°œë…
- [ ] í† í¬ë‚˜ì´ì œì´ì…˜ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] BPEì˜ ì •ì˜ë¥¼ ì•Œê³  ìˆë‹¤
- [ ] OOV ë¬¸ì œë¥¼ ì´í•´í–ˆë‹¤
- [ ] BPE ì•Œê³ ë¦¬ì¦˜ 4ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ë§í•  ìˆ˜ ìˆë‹¤
- [ ] ì¢…ì´ì— ì†ìœ¼ë¡œ BPE ì˜ˆì‹œë¥¼ í’€ì–´ë´¤ë‹¤

### Week 2: ì•Œê³ ë¦¬ì¦˜ ì´í•´
- [ ] ì¸ì½”ë”© ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆë‹¤
- [ ] ë””ì½”ë”© ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆë‹¤
- [ ] í•„ìš”í•œ ìë£Œêµ¬ì¡° 3ê°€ì§€ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] ì™•ë³µ í…ŒìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì•ˆë‹¤
- [ ] ë°”ì´íŠ¸ ì¸ì½”ë”©ì„ ì´í•´í–ˆë‹¤

### Week 3: ê¸°ë³¸ êµ¬í˜„
- [ ] base.pyë¥¼ ì‘ì„±í–ˆë‹¤
- [ ] BasicTokenizer í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì—ˆë‹¤
- [ ] _get_stats í•¨ìˆ˜ë¥¼ êµ¬í˜„í–ˆë‹¤
- [ ] _merge í•¨ìˆ˜ë¥¼ êµ¬í˜„í–ˆë‹¤
- [ ] ê° í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•´ë´¤ë‹¤

### Week 4: í•µì‹¬ êµ¬í˜„
- [ ] train ë©”ì„œë“œë¥¼ ì™„ì„±í–ˆë‹¤
- [ ] encode ë©”ì„œë“œë¥¼ ì™„ì„±í–ˆë‹¤
- [ ] decode ë©”ì„œë“œë¥¼ ì™„ì„±í–ˆë‹¤
- [ ] ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ ì „ì²´ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í–ˆë‹¤
- [ ] ì™•ë³µ í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•œë‹¤

### Week 5: ê³ ê¸‰ ê¸°ëŠ¥
- [ ] save ë©”ì„œë“œë¥¼ êµ¬í˜„í–ˆë‹¤
- [ ] load ë©”ì„œë“œë¥¼ êµ¬í˜„í–ˆë‹¤
- [ ] JSON ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°ë¥¼ í…ŒìŠ¤íŠ¸í–ˆë‹¤
- [ ] (ì„ íƒ) RegexBPEë¥¼ ì´í•´í–ˆë‹¤
- [ ] (ì„ íƒ) í° í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ í›ˆë ¨í•´ë´¤ë‹¤

### ìµœì¢… í™•ì¸
- [ ] ì „ì²´ ì½”ë“œê°€ ì—ëŸ¬ ì—†ì´ ì‹¤í–‰ëœë‹¤
- [ ] ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸í•´ë´¤ë‹¤
- [ ] ì½”ë“œë¥¼ ë‹¤ë¥¸ ì‚¬ëŒì—ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] BPEì˜ í•œê³„ë¥¼ ì´í•´í•˜ê³  ìˆë‹¤
- [ ] ë‹¤ìŒ ë‹¨ê³„ (RegexBPE, sentencepiece ë“±)ë¥¼ ì•Œê³  ìˆë‹¤

---

## í•™ìŠµ íŒ

### ë§‰í ë•Œ ëŒ€ì²˜ë²•

1. **ì—ëŸ¬ê°€ ë‚˜ë©´**
   - ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì²œì²œíˆ ì½ê¸°
   - ì–´ëŠ ì¤„ì—ì„œ ì—ëŸ¬ê°€ ë‚¬ëŠ”ì§€ í™•ì¸
   - print()ë¡œ ì¤‘ê°„ ê°’ ì¶œë ¥í•´ë³´ê¸°
   - í•¨ìˆ˜ë¥¼ ì‘ì€ ì˜ˆì‹œë¡œ ë”°ë¡œ í…ŒìŠ¤íŠ¸

2. **ì´í•´ê°€ ì•ˆ ë˜ë©´**
   - ì¢…ì´ì— ê·¸ë¦¼ ê·¸ë¦¬ê¸°
   - ë” ì‘ì€ ì˜ˆì‹œë¡œ ì‹œì‘í•˜ê¸°
   - í•˜ë£¨ ì‰¬ê³  ë‹¤ì‹œ ë³´ê¸°
   - ê°œë…ì„ ìì‹ ì˜ ë§ë¡œ ì„¤ëª…í•´ë³´ê¸°

3. **ì§€ë£¨í•˜ë©´**
   - ì ì‹œ ì‰¬ê¸°
   - ë‹¤ë¥¸ ì˜ˆì œ ì‹œë„í•´ë³´ê¸°
   - YouTube ì˜ìƒ ë³´ê¸°
   - ì™œ ì´ê±¸ ë°°ìš°ëŠ”ì§€ ìƒê¸°í•˜ê¸°

### íš¨ê³¼ì ì¸ í•™ìŠµ ë°©ë²•

1. **ëŠ¥ë™ì  í•™ìŠµ**
   - ë³µì‚¬-ë¶™ì—¬ë„£ê¸° ê¸ˆì§€!
   - ì§ì ‘ íƒ€ì´í•‘í•˜ë©° ì´í•´í•˜ê¸°
   - ë³€í˜•í•´ì„œ ì‹¤í—˜í•´ë³´ê¸°

2. **ë°˜ë³µ í•™ìŠµ**
   - í•˜ë£¨ ë’¤ ë³µìŠµí•˜ê¸°
   - ì¼ì£¼ì¼ ë’¤ ë‹¤ì‹œ êµ¬í˜„í•´ë³´ê¸°
   - ë‹¤ë¥¸ ì‚¬ëŒì—ê²Œ ì„¤ëª…í•´ë³´ê¸°

3. **í”„ë¡œì íŠ¸ í™•ì¥**
   - ìì‹ ë§Œì˜ ê¸°ëŠ¥ ì¶”ê°€í•˜ê¸°
   - ì‹œê°í™” ë„êµ¬ ë§Œë“¤ê¸°
   - ì„±ëŠ¥ ì¸¡ì •í•´ë³´ê¸°

---

## ë‹¤ìŒ ë‹¨ê³„

BPEë¥¼ ë§ˆìŠ¤í„°í•œ í›„ì—ëŠ”:

1. **SentencePiece** ê³µë¶€í•˜ê¸°
2. **WordPiece** ì´í•´í•˜ê¸°
3. **Tiktoken** (OpenAI í† í¬ë‚˜ì´ì €) ë¶„ì„í•˜ê¸°
4. **Tokenizer ë²¤ì¹˜ë§ˆí¬** ì‹¤í—˜í•˜ê¸°
5. **ìì‹ ë§Œì˜ ì–¸ì–´ ëª¨ë¸** ë§Œë“¤ì–´ë³´ê¸°

---

## ë§ˆì¹˜ë©°

BPEëŠ” í˜„ëŒ€ NLPì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.
ì²˜ìŒì—ëŠ” ì–´ë ¤ìš¸ ìˆ˜ ìˆì§€ë§Œ, ì²œì²œíˆ í•œ ê±¸ìŒì”© ë‚˜ì•„ê°€ë©´ ë°˜ë“œì‹œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**í¬ê¸°í•˜ì§€ ë§ˆì„¸ìš”!**
ëª¨ë“  ì „ë¬¸ê°€ë„ ì²˜ìŒì—ëŠ” ì´ˆë³´ìì˜€ìŠµë‹ˆë‹¤.

**ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!**
ë°°ì›€ì—ëŠ” ë¶€ë„ëŸ¬ì›€ì´ ì—†ìŠµë‹ˆë‹¤.

**í–‰ìš´ì„ ë¹•ë‹ˆë‹¤!** ğŸš€

---

**ì‘ì„±ì¼**: 2026-01-07
**ë²„ì „**: 1.0
**ëŒ€ìƒ**: BPE ì™„ì „ ì´ˆë³´ì
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 5ì£¼ (ì£¼ë‹¹ 5-7ì‹œê°„)
